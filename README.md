# Gemini AI Parenting Assistant

> _â€œAsk me anything about parenting â€” Iâ€™ll respond like a calm, supportive coach.â€_

An interactive AI parenting coach powered by **Google Gemini 2.0 Flash Lite**, built with **Streamlit**, **FastAPI**, and **ChromaDB**.  
It provides personalized, empathetic guidance for parents â€” tailored by a childâ€™s **age group**, **behavior topic**, and conversation tone.

---

## ğŸŒŸ Demo

![App Demo](parenting_project/demo.png)  
_(Example question: â€œMy kid doesnâ€™t like to go to school.â€)_

---

## Features

âœ… Age- and topic-aware parenting answers  
âœ… Real-time AI replies using **Gemini 2.0 Flash Lite**  
âœ… Emotional tone adaptation via **VADER Sentiment Analysis**  
âœ… Cognitive-intent tuning using **Bloomâ€™s Taxonomy**  
âœ… Local knowledge retrieval with **ChromaDB**  
âœ… Seamless backend powered by **FastAPI + Render**

---

<details>
<summary><b>Tech Stack Overview</b></summary>

| Layer          | Tools                                         |
| -------------- | --------------------------------------------- |
| **Frontend**   | Streamlit                                     |
| **Backend**    | FastAPI                                       |
| **Model**      | Google Gemini 2.0 Flash Lite                  |
| **Database**   | ChromaDB                                      |
| **NLP**        | VADER Sentiment Analyzer                      |
| **Env Mgmt**   | dotenv                                        |
| **Deployment** | Render (backend) + Streamlit Cloud (frontend) |

</details>

---

## Deployment

| **Component**       | **Platform**                                    |
| ------------------- | ----------------------------------------------- |
| **Backend**         | [Render â†—](https://render.com)                  |
| **Frontend**        | [Streamlit Cloud â†—](https://streamlit.io/cloud) |
| **Environment Var** | `GOOGLE_API_KEY` â€” set in Render Settings       |

---

<details> <summary><b>Performance Tips</b></summary>

Use "gemini-2.0-flash-lite" for fastest responses

Keep prompt context under â‰ˆ 500 characters

Run Chroma locally for low-latency retrieval

## Toggle local vs backend inference:

## Author

**Nicole Chen**  
AI & Data Science | University of Michigan
[LinkedIn â†—](https://www.linkedin.com/in/nicoolesy)â€ƒ| [GitHub â†—](https://github.com/nicoolesy)

> ğŸ§¡ _Built to help parents understand, connect, and grow together â€” one gentle conversation at a time._
